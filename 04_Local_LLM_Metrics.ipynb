{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local LLM Metrics: Latency, Throughput, and Parameters\n",
    "\n",
    "In the previous notebooks, we explored metrics for Classification and Regression. In the world of Generative AI and Large Language Models (LLMs), accuracy is harder to quantify mathematically. \n",
    "\n",
    "Instead, AI Project Managers must focus on **Operational Metrics** and **Behavioral Parameters**.\n",
    "\n",
    "## Goals\n",
    "In this notebook, you will not just run a model; you will measure its performance characteristics.\n",
    "1. **Connect** to your local Ollama instance via Python.\n",
    "2. **Measure Latency:** How long does the user wait?\n",
    "3. **Measure Throughput:** How fast is text generated (Tokens per Second)?\n",
    "4. **Experiment with Temperature:** How does randomness affect output?\n",
    "\n",
    "### Prerequisites\n",
    "Ensure Ollama is running in your terminal (`ollama serve` or the desktop app) and you have pulled a model (e.g., `ollama pull llama3.2:3b`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The API Connection\n",
    "\n",
    "Unlike the previous notebooks where we used `scikit-learn` to run calculations internally, LLMs usually run as a separate **Service**. We communicate with them using HTTP requests (APIs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "MODEL_NAME = \"llama3.2:3b\"  # Change if you want to use a different model\n",
    "\n",
    "def check_server_status():\n",
    "    try:\n",
    "        # Simple GET request to see if the server is up\n",
    "        response = requests.get(\"http://localhost:11434/\")\n",
    "        if response.status_code == 200:\n",
    "            print(\"âœ… Ollama Server is running!\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ Server returned status: {response.status_code}\")\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"âŒ Could not connect to Ollama. Is the app running?\")\n",
    "\n",
    "check_server_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a persistent session to reuse connections for all requests\n",
    "session = requests.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Measuring Latency and Throughput\n",
    "\n",
    "For a PM, \"Speed\" means two different things:\n",
    "\n",
    "1. **Latency (Time to First Token):** How long before the AI *starts* writing? (Crucial for Chatbots).\n",
    "2. **Throughput (Tokens per Second):** How fast does it write the whole answer? (Crucial for summarizing large documents).\n",
    "\n",
    "Let's write a function to measure this. We will send a `POST` request to the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "def query_model_with_metrics(prompt, model=MODEL_NAME):\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    # Default values (safe fallbacks)\n",
    "    text = \"\"\n",
    "    token_count = 0\n",
    "    error = None\n",
    "    \n",
    "    # Start Timer\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    try:\n",
    "        # CHANGE HERE: Use 'session.post' instead of 'requests.post'\n",
    "        response = session.post(OLLAMA_URL, json=payload)\n",
    "        response.raise_for_status()  \n",
    "        \n",
    "        # Parse logic\n",
    "        data = response.json()\n",
    "        text = data.get(\"response\", \"\")\n",
    "        token_count = data.get(\"eval_count\", 0)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        error = f\"Network/API Error: {e}\"\n",
    "    except ValueError:\n",
    "        error = \"Invalid JSON received from server\"\n",
    "    except Exception as e:\n",
    "        error = f\"Unexpected Error: {e}\"\n",
    "\n",
    "    # Stop Timer\n",
    "    duration = time.perf_counter() - start_time\n",
    "    \n",
    "    # Calculate throughput (avoid division by zero)\n",
    "    tps = token_count / duration if duration > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"duration_seconds\": duration,\n",
    "        \"token_count\": token_count,\n",
    "        \"tokens_per_sec\": tps,\n",
    "        \"error\": error\n",
    "    }\n",
    "\n",
    "# Test it out\n",
    "metrics = query_model_with_metrics(\"Explain Quantum Computing to a 5 year old.\")\n",
    "\n",
    "if metrics.get(\"error\"):\n",
    "    print(f\"Error: {metrics['error']}\")\n",
    "else:\n",
    "    print(f\"Response: {metrics['text'][:200]}...\")\n",
    "    print(f\"\\nâ±ï¸ Total Duration: {metrics['duration_seconds']:.2f}s\")\n",
    "    print(f\"ðŸš€ Speed: {metrics['tokens_per_sec']:.2f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "Look at your `tokens/sec`. \n",
    "* **Human reading speed:** ~3-5 tokens/second.\n",
    "* **Typical GPU speed:** >50 tokens/second.\n",
    "* **Typical CPU (Laptop) speed:** 5-20 tokens/second.\n",
    "\n",
    "If your local number is lower than human reading speed, the User Experience (UX) will feel \"laggy.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Temperature Parameter\n",
    "\n",
    "In the `Classification Metrics` notebook, we discussed thresholds (0.5 vs 0.25) changing the outcome. In LLMs, the equivalent lever is **Temperature**.\n",
    "\n",
    "* **Temperature 0.0:** Deterministic. Always picks the most likely next word. Good for coding, data extraction.\n",
    "* **Temperature 0.8+:** Creative. Picks less likely words. Good for poetry, brainstorming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_with_temp(prompt, temperature):\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\"temperature\": temperature}\n",
    "    }\n",
    "    try:\n",
    "        response = session.post(OLLAMA_URL, json=payload)\n",
    "        response.raise_for_status() # Raises error for 4xx/5xx codes\n",
    "        return response.json().get(\"response\", \"\")\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "test_prompt = \"Propose 3 unique names for a new coffee shop in Berlin.\"\n",
    "print(query_with_temp(test_prompt, 0.0))\n",
    "print(query_with_temp(test_prompt, 0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. System Prompts (The Persona)\n",
    "\n",
    "Just as we set up requirements before coding, we set up **System Prompts** before the user interacts with the AI. This guides the model's behavior throughout the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_with_system_prompt(user_input, system_role):\n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": user_input,\n",
    "        \"system\": system_role,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = session.post(OLLAMA_URL, json=payload)\n",
    "        response.raise_for_status() # Catches 404s/500s\n",
    "        return response.json().get(\"response\", \"\")\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "user_q = \"I made a mistake in production.\"\n",
    "\n",
    "# Scenario A: The Empathetic Manager\n",
    "role_a = \"You are an empathetic career coach. Your tone is soothing and supportive.\"\n",
    "print(f\"\\n[Coach]: {query_with_system_prompt(user_q, role_a)}\")\n",
    "\n",
    "# Scenario B: The Strict Technical Lead\n",
    "role_b = \"You are a strict Senior DevOps Engineer. You focus only on root cause analysis and immediate fixes. Be brief.\"\n",
    "print(f\"\\n[DevOps]: {query_with_system_prompt(user_q, role_b)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You have now successfully:\n",
    "1. Built a basic **LLM Client** in Python.\n",
    "2. Calculated **Throughput** (a key cost/performance driver).\n",
    "3. Manipulated **Temperature** to change creativity.\n",
    "4. Used **System Prompts** to engineer specific behaviors.\n",
    "\n",
    "### Next Step\n",
    "Try integrating this script into a loop to build a CLI Chatbot!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
